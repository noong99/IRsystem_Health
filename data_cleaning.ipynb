{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9116d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    query-id  corpus-id  score\n",
      "0  7665777-1   32320506      1\n",
      "1  7665777-1   32293716      1\n",
      "2  7665777-1   23219649      1\n",
      "3  7665777-1   30339549      1\n",
      "4  7665777-1   17470624      1\n",
      "[1 2]\n",
      "(1978118, 3)\n",
      "(77147, 3)\n",
      "(73394, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "qrels_train = pd.read_csv(\"data/PAR/qrels_train.tsv\", sep=\"\\t\")\n",
    "qrels_test = pd.read_csv(\"data/PAR/qrels_test.tsv\", sep=\"\\t\")\n",
    "qrels_dev = pd.read_csv(\"data/PAR/qrels_dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(qrels_train.head())\n",
    "\n",
    "print(qrels_train[\"score\"].unique())\n",
    "\n",
    "print(qrels_train.shape)\n",
    "print(qrels_test.shape)\n",
    "print(qrels_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c57131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new corpus\n",
    "# 50k train \n",
    "# top 30 articles from BM25 retrieval results for each query as negative samples\n",
    "\n",
    "# 1. Randomly select 50k articles from the corpus and queries\n",
    "# 2. Fore each query, 30 irrelevant articles(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27ad2177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique documents in qrels_train: 879083\n",
      "Unique documents in qrels_dev: 55265\n",
      "Unique documents in qrels_test: 56304\n",
      "Sampled 80000 unique documents and saved 180770 rows to data/PAR/80k_qrels_train.tsv\n",
      "Sampled 10000 unique documents and saved 13440 rows to data/PAR/10k_qrels_dev.tsv\n",
      "Sampled 10000 unique documents and saved 13666 rows to data/PAR/10k_qrels_test.tsv\n",
      "\n",
      "Number of unique queries: 95914\n",
      "Number of unique documents in saved file: 80000\n",
      "\n",
      "Number of unique queries: 4841\n",
      "Number of unique documents in saved file: 10000\n",
      "\n",
      "Number of unique queries: 4868\n",
      "Number of unique documents in saved file: 10000\n"
     ]
    }
   ],
   "source": [
    "# Sample 50k unique articles (distinct corpus-id) from qrels_train.tsv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Read the original qrels_train.tsv\n",
    "qrels_train = pd.read_csv(\"data/PAR/qrels_train.tsv\", sep=\"\\t\")\n",
    "qrels_dev = pd.read_csv(\"data/PAR/qrels_dev.tsv\", sep=\"\\t\")\n",
    "qrels_test = pd.read_csv(\"data/PAR/qrels_test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Get unique document IDs available in qrels\n",
    "unique_docs_train = qrels_train['corpus-id'].unique()\n",
    "unique_docs_dev = qrels_dev['corpus-id'].unique()\n",
    "unique_docs_test = qrels_test['corpus-id'].unique()\n",
    "print(f\"Unique documents in qrels_train: {len(unique_docs_train)}\")\n",
    "print(f\"Unique documents in qrels_dev: {len(unique_docs_dev)}\")\n",
    "print(f\"Unique documents in qrels_test: {len(unique_docs_test)}\")\n",
    "\n",
    "# Decide sample size (don't request more than exist)\n",
    "n_to_sample_train = min(80000, len(unique_docs_train))\n",
    "n_to_sample_dev = min(10000, len(unique_docs_dev))\n",
    "n_to_sample_test = min(10000, len(unique_docs_test))\n",
    "\n",
    "# Sample unique document IDs without replacement\n",
    "sampled_docs_train = np.random.choice(unique_docs_train, size=n_to_sample_train, replace=False)\n",
    "sampled_docs_dev = np.random.choice(unique_docs_dev, size=n_to_sample_dev, replace=False)\n",
    "sampled_docs_test = np.random.choice(unique_docs_test, size=n_to_sample_test, replace=False)\n",
    "\n",
    "# Keep all qrels rows that reference the sampled documents\n",
    "sampled_qrels_train = qrels_train[qrels_train['corpus-id'].isin(sampled_docs_train)]\n",
    "sampled_qrels_dev = qrels_dev[qrels_dev['corpus-id'].isin(sampled_docs_dev)]\n",
    "sampled_qrels_test = qrels_test[qrels_test['corpus-id'].isin(sampled_docs_test)]\n",
    "\n",
    "# Save only the original query pair + scores in fixed column order\n",
    "cols = ['query-id', 'corpus-id', 'score']\n",
    "output_path_train = \"data/PAR/80k_qrels_train.tsv\"\n",
    "output_path_dev = \"data/PAR/10k_qrels_dev.tsv\"\n",
    "output_path_test = \"data/PAR/10k_qrels_test.tsv\"\n",
    "\n",
    "sampled_qrels_train[cols].to_csv(output_path_train, sep='\\t', index=False)\n",
    "sampled_qrels_dev[cols].to_csv(output_path_dev, sep='\\t', index=False)\n",
    "sampled_qrels_test[cols].to_csv(output_path_test, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Sampled {n_to_sample_train} unique documents and saved {sampled_qrels_train.shape[0]} rows to {output_path_train}\")\n",
    "print(f\"Sampled {n_to_sample_dev} unique documents and saved {sampled_qrels_dev.shape[0]} rows to {output_path_dev}\")\n",
    "print(f\"Sampled {n_to_sample_test} unique documents and saved {sampled_qrels_test.shape[0]} rows to {output_path_test}\")\n",
    "\n",
    "# Optional: save the list of sampled document ids for reproducibility\n",
    "# pd.Series(sampled_docs).to_csv('data/PAR/50k_sampled_doc_ids.txt', index=False, header=False)\n",
    "# print(\"Saved sampled document ids to data/PAR/50k_sampled_doc_ids.txt\")\n",
    "\n",
    "# Check if we maintained the same columns and structure\n",
    "# print(\"\\nFirst few rows of sampled data:\")\n",
    "# print(sampled_qrels_train.head())\n",
    "\n",
    "# Check unique queries and documents in the sample\n",
    "print(f\"\\nNumber of unique queries: {sampled_qrels_train['query-id'].nunique()}\")\n",
    "print(f\"Number of unique documents in saved file: {sampled_qrels_train['corpus-id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nNumber of unique queries: {sampled_qrels_dev['query-id'].nunique()}\")\n",
    "print(f\"Number of unique documents in saved file: {sampled_qrels_dev['corpus-id'].nunique()}\")\n",
    "\n",
    "print(f\"\\nNumber of unique queries: {sampled_qrels_test['query-id'].nunique()}\")\n",
    "print(f\"Number of unique documents in saved file: {sampled_qrels_test['corpus-id'].nunique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69469785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 14:37:23,469 - Starting corpus extraction process\n",
      "2025-11-01 14:37:23,471 - Reading qrels files...\n",
      "2025-11-01 14:37:23,586 - Found 80000 unique documents in train\n",
      "2025-11-01 14:37:23,586 - Found 10000 unique documents in dev\n",
      "2025-11-01 14:37:23,587 - Found 10000 unique documents in test\n",
      "2025-11-01 14:37:23,600 - Total unique documents needed: 98687\n",
      "2025-11-01 14:37:23,628 - Reading corpus.jsonl and extracting required documents...\n",
      "2025-11-01 14:37:29,393 - Processed 1,000,000 documents...\n",
      "2025-11-01 14:37:34,872 - Processed 2,000,000 documents...\n",
      "2025-11-01 14:37:40,363 - Processed 3,000,000 documents...\n",
      "2025-11-01 14:37:45,830 - Processed 4,000,000 documents...\n",
      "2025-11-01 14:37:51,317 - Processed 5,000,000 documents...\n",
      "2025-11-01 14:37:56,778 - Processed 6,000,000 documents...\n",
      "2025-11-01 14:38:02,278 - Processed 7,000,000 documents...\n",
      "2025-11-01 14:38:07,851 - Processed 8,000,000 documents...\n",
      "2025-11-01 14:38:13,314 - Processed 9,000,000 documents...\n",
      "2025-11-01 14:38:18,630 - Processed 10,000,000 documents...\n",
      "2025-11-01 14:38:23,984 - Processed 11,000,000 documents...\n",
      "2025-11-01 14:38:27,924 - Finished processing 11,713,201 documents\n",
      "2025-11-01 14:38:27,925 - Found 98,687 matching documents\n",
      "2025-11-01 14:38:27,925 - Saving matched documents to data/PAR/sampled_corpus.jsonl\n",
      "2025-11-01 14:38:28,966 - ✅ Extraction complete!\n",
      "2025-11-01 14:38:28,966 - Full log saved to data/PAR/corpus_extraction.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Total documents processed: 11,713,201\n",
      "Documents in train_50k: 80,000\n",
      "Documents in dev: 10,000\n",
      "Documents in test: 10,000\n",
      "Total unique documents needed: 98,687\n",
      "Documents extracted and saved: 98,687\n",
      "\n",
      "Detailed log saved to: data/PAR/corpus_extraction.log\n"
     ]
    }
   ],
   "source": [
    "# Combine unique corpus IDs from train/dev/test and extract documents\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up logging\n",
    "log_path = \"data/PAR/corpus_extraction.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_path),\n",
    "        logging.StreamHandler()  # Also print to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Log start time\n",
    "logger.info(\"Starting corpus extraction process\")\n",
    "\n",
    "# Read and combine unique corpus IDs from all qrels files\n",
    "logger.info(\"Reading qrels files...\")\n",
    "\n",
    "train_50k = pd.read_csv(\"data/PAR/80k_qrels_train.tsv\", sep=\"\\t\")\n",
    "dev = pd.read_csv(\"data/PAR/10k_qrels_dev.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"data/PAR/10k_qrels_test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Get unique corpus IDs from each file\n",
    "train_ids = set(train_50k['corpus-id'].unique())\n",
    "dev_ids = set(dev['corpus-id'].unique())\n",
    "test_ids = set(test['corpus-id'].unique())\n",
    "\n",
    "logger.info(f\"Found {len(train_ids)} unique documents in train\")\n",
    "logger.info(f\"Found {len(dev_ids)} unique documents in dev\")\n",
    "logger.info(f\"Found {len(test_ids)} unique documents in test\")\n",
    "\n",
    "# Combine all unique IDs\n",
    "all_ids = train_ids.union(dev_ids, test_ids)\n",
    "logger.info(f\"Total unique documents needed: {len(all_ids)}\")\n",
    "\n",
    "# Convert IDs to strings for matching (in case they're numeric)\n",
    "all_ids = {str(id) for id in all_ids}\n",
    "\n",
    "# Read and filter corpus.jsonl\n",
    "logger.info(\"Reading corpus.jsonl and extracting required documents...\")\n",
    "\n",
    "output_docs = []\n",
    "total_docs = 0\n",
    "matched_docs = 0\n",
    "\n",
    "with open(\"data/PAR/corpus.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        total_docs += 1\n",
    "        if total_docs % 1000000 == 0:  # Log progress every million documents\n",
    "            logger.info(f\"Processed {total_docs:,} documents...\")\n",
    "        \n",
    "        if line.strip():\n",
    "            doc = json.loads(line)\n",
    "            if str(doc['_id']) in all_ids:\n",
    "                output_docs.append(doc)\n",
    "                matched_docs += 1\n",
    "\n",
    "logger.info(f\"Finished processing {total_docs:,} documents\")\n",
    "logger.info(f\"Found {matched_docs:,} matching documents\")\n",
    "\n",
    "# Save filtered corpus\n",
    "output_path = \"data/PAR/sampled_corpus.jsonl\"\n",
    "logger.info(f\"Saving matched documents to {output_path}\")\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for doc in output_docs:\n",
    "        f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "logger.info(\"✅ Extraction complete!\")\n",
    "logger.info(f\"Full log saved to {log_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total documents processed: {total_docs:,}\")\n",
    "print(f\"Documents in train_50k: {len(train_ids):,}\")\n",
    "print(f\"Documents in dev: {len(dev_ids):,}\")\n",
    "print(f\"Documents in test: {len(test_ids):,}\")\n",
    "print(f\"Total unique documents needed: {len(all_ids):,}\")\n",
    "print(f\"Documents extracted and saved: {matched_docs:,}\")\n",
    "print(f\"\\nDetailed log saved to: {log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea667371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 14:50:45,085 - Loading existing sampled corpus from data/PAR/sampled_corpus.jsonl\n",
      "2025-11-01 14:50:45,677 - Loaded 98,687 documents; unique IDs: 98,687\n",
      "2025-11-01 14:50:45,677 - Current sampled corpus size: 98,687. Need 101,313 more documents to reach 200,000.\n",
      "2025-11-01 14:50:45,678 - Starting reservoir sampling from data/PAR/irrelevant.jsonl for k=101313 (excluding 98,687 ids)\n",
      "2025-11-01 14:50:46,311 - Seen 100,000 eligible irrelevant docs so far (processed 100,000 lines)\n",
      "2025-11-01 14:50:47,106 - Seen 200,000 eligible irrelevant docs so far (processed 200,000 lines)\n",
      "2025-11-01 14:50:47,859 - Seen 300,000 eligible irrelevant docs so far (processed 300,000 lines)\n",
      "2025-11-01 14:50:48,608 - Seen 400,000 eligible irrelevant docs so far (processed 400,000 lines)\n",
      "2025-11-01 14:50:49,285 - Seen 500,000 eligible irrelevant docs so far (processed 500,000 lines)\n",
      "2025-11-01 14:50:50,043 - Seen 600,000 eligible irrelevant docs so far (processed 600,000 lines)\n",
      "2025-11-01 14:50:50,736 - Seen 700,000 eligible irrelevant docs so far (processed 700,000 lines)\n",
      "2025-11-01 14:50:51,426 - Seen 800,000 eligible irrelevant docs so far (processed 800,000 lines)\n",
      "2025-11-01 14:50:52,129 - Seen 900,000 eligible irrelevant docs so far (processed 900,000 lines)\n",
      "2025-11-01 14:50:52,808 - Seen 1,000,000 eligible irrelevant docs so far (processed 1,000,000 lines)\n",
      "2025-11-01 14:50:53,508 - Seen 1,100,000 eligible irrelevant docs so far (processed 1,100,000 lines)\n",
      "2025-11-01 14:50:54,199 - Seen 1,200,000 eligible irrelevant docs so far (processed 1,200,000 lines)\n",
      "2025-11-01 14:50:54,832 - Seen 1,300,000 eligible irrelevant docs so far (processed 1,300,000 lines)\n",
      "2025-11-01 14:50:55,496 - Seen 1,400,000 eligible irrelevant docs so far (processed 1,400,000 lines)\n",
      "2025-11-01 14:50:56,156 - Seen 1,500,000 eligible irrelevant docs so far (processed 1,500,000 lines)\n",
      "2025-11-01 14:50:56,799 - Seen 1,600,000 eligible irrelevant docs so far (processed 1,600,000 lines)\n",
      "2025-11-01 14:50:57,508 - Seen 1,700,000 eligible irrelevant docs so far (processed 1,700,000 lines)\n",
      "2025-11-01 14:50:58,182 - Seen 1,800,000 eligible irrelevant docs so far (processed 1,800,000 lines)\n",
      "2025-11-01 14:50:58,878 - Seen 1,900,000 eligible irrelevant docs so far (processed 1,900,000 lines)\n",
      "2025-11-01 14:50:59,498 - Seen 2,000,000 eligible irrelevant docs so far (processed 2,000,000 lines)\n",
      "2025-11-01 14:51:00,144 - Seen 2,100,000 eligible irrelevant docs so far (processed 2,100,000 lines)\n",
      "2025-11-01 14:51:00,839 - Seen 2,200,000 eligible irrelevant docs so far (processed 2,200,000 lines)\n",
      "2025-11-01 14:51:01,471 - Seen 2,300,000 eligible irrelevant docs so far (processed 2,300,000 lines)\n",
      "2025-11-01 14:51:02,133 - Seen 2,400,000 eligible irrelevant docs so far (processed 2,400,000 lines)\n",
      "2025-11-01 14:51:02,832 - Seen 2,500,000 eligible irrelevant docs so far (processed 2,500,000 lines)\n",
      "2025-11-01 14:51:03,508 - Seen 2,600,000 eligible irrelevant docs so far (processed 2,600,000 lines)\n",
      "2025-11-01 14:51:04,226 - Seen 2,700,000 eligible irrelevant docs so far (processed 2,700,000 lines)\n",
      "2025-11-01 14:51:04,884 - Seen 2,800,000 eligible irrelevant docs so far (processed 2,800,000 lines)\n",
      "2025-11-01 14:51:05,592 - Seen 2,900,000 eligible irrelevant docs so far (processed 2,900,000 lines)\n",
      "2025-11-01 14:51:06,263 - Seen 3,000,000 eligible irrelevant docs so far (processed 3,000,000 lines)\n",
      "2025-11-01 14:51:06,930 - Seen 3,100,000 eligible irrelevant docs so far (processed 3,100,000 lines)\n",
      "2025-11-01 14:51:07,611 - Seen 3,200,000 eligible irrelevant docs so far (processed 3,200,000 lines)\n",
      "2025-11-01 14:51:08,278 - Seen 3,300,000 eligible irrelevant docs so far (processed 3,300,000 lines)\n",
      "2025-11-01 14:51:09,001 - Seen 3,400,000 eligible irrelevant docs so far (processed 3,400,000 lines)\n",
      "2025-11-01 14:51:09,684 - Seen 3,500,000 eligible irrelevant docs so far (processed 3,500,000 lines)\n",
      "2025-11-01 14:51:10,375 - Seen 3,600,000 eligible irrelevant docs so far (processed 3,600,000 lines)\n",
      "2025-11-01 14:51:11,027 - Seen 3,700,000 eligible irrelevant docs so far (processed 3,700,000 lines)\n",
      "2025-11-01 14:51:11,752 - Seen 3,800,000 eligible irrelevant docs so far (processed 3,800,000 lines)\n",
      "2025-11-01 14:51:12,441 - Seen 3,900,000 eligible irrelevant docs so far (processed 3,900,000 lines)\n",
      "2025-11-01 14:51:13,096 - Seen 4,000,000 eligible irrelevant docs so far (processed 4,000,000 lines)\n",
      "2025-11-01 14:51:13,785 - Seen 4,100,000 eligible irrelevant docs so far (processed 4,100,000 lines)\n",
      "2025-11-01 14:51:14,395 - Seen 4,200,000 eligible irrelevant docs so far (processed 4,200,000 lines)\n",
      "2025-11-01 14:51:15,085 - Seen 4,300,000 eligible irrelevant docs so far (processed 4,300,000 lines)\n",
      "2025-11-01 14:51:15,760 - Seen 4,400,000 eligible irrelevant docs so far (processed 4,400,000 lines)\n",
      "2025-11-01 14:51:16,413 - Seen 4,500,000 eligible irrelevant docs so far (processed 4,500,000 lines)\n",
      "2025-11-01 14:51:17,097 - Seen 4,600,000 eligible irrelevant docs so far (processed 4,600,000 lines)\n",
      "2025-11-01 14:51:17,781 - Seen 4,700,000 eligible irrelevant docs so far (processed 4,700,000 lines)\n",
      "2025-11-01 14:51:18,456 - Seen 4,800,000 eligible irrelevant docs so far (processed 4,800,000 lines)\n",
      "2025-11-01 14:51:19,097 - Seen 4,900,000 eligible irrelevant docs so far (processed 4,900,000 lines)\n",
      "2025-11-01 14:51:19,784 - Seen 5,000,000 eligible irrelevant docs so far (processed 5,000,000 lines)\n",
      "2025-11-01 14:51:20,467 - Seen 5,100,000 eligible irrelevant docs so far (processed 5,100,000 lines)\n",
      "2025-11-01 14:51:21,123 - Seen 5,200,000 eligible irrelevant docs so far (processed 5,200,000 lines)\n",
      "2025-11-01 14:51:21,723 - Seen 5,300,000 eligible irrelevant docs so far (processed 5,300,000 lines)\n",
      "2025-11-01 14:51:22,423 - Seen 5,400,000 eligible irrelevant docs so far (processed 5,400,000 lines)\n",
      "2025-11-01 14:51:23,083 - Seen 5,500,000 eligible irrelevant docs so far (processed 5,500,000 lines)\n",
      "2025-11-01 14:51:23,806 - Seen 5,600,000 eligible irrelevant docs so far (processed 5,600,000 lines)\n",
      "2025-11-01 14:51:24,431 - Seen 5,700,000 eligible irrelevant docs so far (processed 5,700,000 lines)\n",
      "2025-11-01 14:51:25,106 - Seen 5,800,000 eligible irrelevant docs so far (processed 5,800,000 lines)\n",
      "2025-11-01 14:51:25,739 - Seen 5,900,000 eligible irrelevant docs so far (processed 5,900,000 lines)\n",
      "2025-11-01 14:51:26,406 - Seen 6,000,000 eligible irrelevant docs so far (processed 6,000,000 lines)\n",
      "2025-11-01 14:51:27,125 - Seen 6,100,000 eligible irrelevant docs so far (processed 6,100,000 lines)\n",
      "2025-11-01 14:51:27,781 - Seen 6,200,000 eligible irrelevant docs so far (processed 6,200,000 lines)\n",
      "2025-11-01 14:51:28,448 - Seen 6,300,000 eligible irrelevant docs so far (processed 6,300,000 lines)\n",
      "2025-11-01 14:51:29,074 - Seen 6,400,000 eligible irrelevant docs so far (processed 6,400,000 lines)\n",
      "2025-11-01 14:51:29,735 - Seen 6,500,000 eligible irrelevant docs so far (processed 6,500,000 lines)\n",
      "2025-11-01 14:51:30,400 - Seen 6,600,000 eligible irrelevant docs so far (processed 6,600,000 lines)\n",
      "2025-11-01 14:51:31,066 - Seen 6,700,000 eligible irrelevant docs so far (processed 6,700,000 lines)\n",
      "2025-11-01 14:51:31,733 - Seen 6,800,000 eligible irrelevant docs so far (processed 6,800,000 lines)\n",
      "2025-11-01 14:51:32,424 - Seen 6,900,000 eligible irrelevant docs so far (processed 6,900,000 lines)\n",
      "2025-11-01 14:51:33,131 - Seen 7,000,000 eligible irrelevant docs so far (processed 7,000,000 lines)\n",
      "2025-11-01 14:51:33,792 - Seen 7,100,000 eligible irrelevant docs so far (processed 7,100,000 lines)\n",
      "2025-11-01 14:51:34,391 - Seen 7,200,000 eligible irrelevant docs so far (processed 7,200,000 lines)\n",
      "2025-11-01 14:51:35,059 - Seen 7,300,000 eligible irrelevant docs so far (processed 7,300,000 lines)\n",
      "2025-11-01 14:51:35,713 - Seen 7,400,000 eligible irrelevant docs so far (processed 7,400,000 lines)\n",
      "2025-11-01 14:51:36,309 - Seen 7,500,000 eligible irrelevant docs so far (processed 7,500,000 lines)\n",
      "2025-11-01 14:51:36,925 - Seen 7,600,000 eligible irrelevant docs so far (processed 7,600,000 lines)\n",
      "2025-11-01 14:51:37,559 - Seen 7,700,000 eligible irrelevant docs so far (processed 7,700,000 lines)\n",
      "2025-11-01 14:51:38,233 - Seen 7,800,000 eligible irrelevant docs so far (processed 7,800,000 lines)\n",
      "2025-11-01 14:51:38,883 - Seen 7,900,000 eligible irrelevant docs so far (processed 7,900,000 lines)\n",
      "2025-11-01 14:51:39,519 - Seen 8,000,000 eligible irrelevant docs so far (processed 8,000,000 lines)\n",
      "2025-11-01 14:51:40,140 - Seen 8,100,000 eligible irrelevant docs so far (processed 8,100,000 lines)\n",
      "2025-11-01 14:51:40,782 - Seen 8,200,000 eligible irrelevant docs so far (processed 8,200,000 lines)\n",
      "2025-11-01 14:51:41,462 - Seen 8,300,000 eligible irrelevant docs so far (processed 8,300,000 lines)\n",
      "2025-11-01 14:51:42,175 - Seen 8,400,000 eligible irrelevant docs so far (processed 8,400,000 lines)\n",
      "2025-11-01 14:51:42,854 - Seen 8,500,000 eligible irrelevant docs so far (processed 8,500,000 lines)\n",
      "2025-11-01 14:51:43,528 - Seen 8,600,000 eligible irrelevant docs so far (processed 8,600,000 lines)\n",
      "2025-11-01 14:51:44,182 - Seen 8,700,000 eligible irrelevant docs so far (processed 8,700,000 lines)\n",
      "2025-11-01 14:51:44,875 - Seen 8,800,000 eligible irrelevant docs so far (processed 8,800,000 lines)\n",
      "2025-11-01 14:51:45,516 - Seen 8,900,000 eligible irrelevant docs so far (processed 8,900,000 lines)\n",
      "2025-11-01 14:51:46,120 - Seen 9,000,000 eligible irrelevant docs so far (processed 9,000,000 lines)\n",
      "2025-11-01 14:51:46,771 - Seen 9,100,000 eligible irrelevant docs so far (processed 9,100,000 lines)\n",
      "2025-11-01 14:51:47,390 - Seen 9,200,000 eligible irrelevant docs so far (processed 9,200,000 lines)\n",
      "2025-11-01 14:51:48,067 - Seen 9,300,000 eligible irrelevant docs so far (processed 9,300,000 lines)\n",
      "2025-11-01 14:51:48,714 - Seen 9,400,000 eligible irrelevant docs so far (processed 9,400,000 lines)\n",
      "2025-11-01 14:51:49,353 - Seen 9,500,000 eligible irrelevant docs so far (processed 9,500,000 lines)\n",
      "2025-11-01 14:51:50,024 - Seen 9,600,000 eligible irrelevant docs so far (processed 9,600,000 lines)\n",
      "2025-11-01 14:51:50,654 - Seen 9,700,000 eligible irrelevant docs so far (processed 9,700,000 lines)\n",
      "2025-11-01 14:51:51,267 - Seen 9,800,000 eligible irrelevant docs so far (processed 9,800,000 lines)\n",
      "2025-11-01 14:51:51,908 - Seen 9,900,000 eligible irrelevant docs so far (processed 9,900,000 lines)\n",
      "2025-11-01 14:51:52,565 - Seen 10,000,000 eligible irrelevant docs so far (processed 10,000,000 lines)\n",
      "2025-11-01 14:51:53,198 - Seen 10,100,000 eligible irrelevant docs so far (processed 10,100,000 lines)\n",
      "2025-11-01 14:51:53,867 - Seen 10,200,000 eligible irrelevant docs so far (processed 10,200,000 lines)\n",
      "2025-11-01 14:51:54,480 - Seen 10,300,000 eligible irrelevant docs so far (processed 10,300,000 lines)\n",
      "2025-11-01 14:51:55,203 - Seen 10,400,000 eligible irrelevant docs so far (processed 10,400,000 lines)\n",
      "2025-11-01 14:51:55,853 - Seen 10,500,000 eligible irrelevant docs so far (processed 10,500,000 lines)\n",
      "2025-11-01 14:51:56,472 - Seen 10,600,000 eligible irrelevant docs so far (processed 10,600,000 lines)\n",
      "2025-11-01 14:51:57,105 - Seen 10,700,000 eligible irrelevant docs so far (processed 10,700,000 lines)\n",
      "2025-11-01 14:51:57,725 - Reservoir sampling finished: seen 10,789,853 eligible candidates, sampled 101,313\n",
      "2025-11-01 14:51:57,726 - Writing combined file to data/PAR/sampled_corpus_200k.jsonl\n",
      "2025-11-01 14:52:00,020 - Wrote 200,000 documents to data/PAR/sampled_corpus_200k.jsonl (existing: 98,687, added: 101,313)\n",
      "2025-11-01 14:52:00,102 - Saved added IDs (101,313) to data/PAR/added_irrelevant_ids.txt\n",
      "2025-11-01 14:52:00,103 - Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Existing docs: 98,687\n",
      "Added docs: 101,313\n",
      "Final total written: 200,000\n",
      "Output file: data/PAR/sampled_corpus_200k.jsonl\n",
      "Log: data/PAR/irrelevant_sampling.log\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Config\n",
    "SAMPLED_CORPUS = Path(\"data/PAR/sampled_corpus.jsonl\")       # existing corpus (98,687)\n",
    "IRRELEVANT = Path(\"data/PAR/irrelevant.jsonl\")               # large irrelevant pool\n",
    "OUTPUT = Path(\"data/PAR/sampled_corpus_200k.jsonl\")          # new combined output\n",
    "ADDED_IDS = Path(\"data/PAR/added_irrelevant_ids.txt\")\n",
    "LOG_PATH = Path(\"data/PAR/irrelevant_sampling.log\")\n",
    "TARGET_TOTAL = 200_000\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(LOG_PATH), logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_existing_docs(path):\n",
    "    docs = []\n",
    "    ids = set()\n",
    "    logger.info(f\"Loading existing sampled corpus from {path}\")\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            doc = json.loads(line)\n",
    "            docs.append(doc)\n",
    "            ids.add(str(doc[\"_id\"]))\n",
    "    logger.info(f\"Loaded {len(docs):,} documents; unique IDs: {len(ids):,}\")\n",
    "    return docs, ids\n",
    "\n",
    "def reservoir_sample_irrelevant(irrelevant_path, exclude_ids, k, seed=None):\n",
    "    \"\"\"\n",
    "    Reservoir-sample k documents from irrelevant_path while skipping docs whose id is in exclude_ids.\n",
    "    Returns list of sampled doc objects (length <= k if not enough candidates).\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        return []\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    reservoir = []\n",
    "    count_candidates = 0\n",
    "    logger.info(f\"Starting reservoir sampling from {irrelevant_path} for k={k} (excluding {len(exclude_ids):,} ids)\")\n",
    "    with irrelevant_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                doc = json.loads(line)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Skipping invalid JSON at line {line_no}: {e}\")\n",
    "                continue\n",
    "            doc_id = str(doc.get(\"_id\") or doc.get(\"id\") or \"\")\n",
    "            if not doc_id or doc_id in exclude_ids:\n",
    "                continue\n",
    "            count_candidates += 1\n",
    "            if len(reservoir) < k:\n",
    "                reservoir.append(doc)\n",
    "            else:\n",
    "                # replace with decreasing probability\n",
    "                j = random.randint(0, count_candidates - 1)\n",
    "                if j < k:\n",
    "                    reservoir[j] = doc\n",
    "            # optional progress log\n",
    "            if count_candidates % 100000 == 0:\n",
    "                logger.info(f\"Seen {count_candidates:,} eligible irrelevant docs so far (processed {line_no:,} lines)\")\n",
    "    logger.info(f\"Reservoir sampling finished: seen {count_candidates:,} eligible candidates, sampled {len(reservoir):,}\")\n",
    "    return reservoir\n",
    "\n",
    "def main():\n",
    "    # 1) load existing sampled corpus\n",
    "    existing_docs, existing_ids = load_existing_docs(SAMPLED_CORPUS)\n",
    "    current_count = len(existing_docs)\n",
    "    need = max(0, TARGET_TOTAL - current_count)\n",
    "    logger.info(f\"Current sampled corpus size: {current_count:,}. Need {need:,} more documents to reach {TARGET_TOTAL:,}.\")\n",
    "\n",
    "    if need == 0:\n",
    "        logger.info(\"Target already reached; copying original file to output.\")\n",
    "        # just copy\n",
    "        with OUTPUT.open(\"w\", encoding=\"utf-8\") as out_f, SAMPLED_CORPUS.open(\"r\", encoding=\"utf-8\") as in_f:\n",
    "            for line in in_f:\n",
    "                out_f.write(line)\n",
    "        return\n",
    "\n",
    "    # 2) reservoir sample from irrelevant.jsonl\n",
    "    sampled_new = reservoir_sample_irrelevant(IRRELEVANT, existing_ids, need, seed=RANDOM_SEED)\n",
    "\n",
    "    # 3) merge and write output\n",
    "    total_written = 0\n",
    "    logger.info(f\"Writing combined file to {OUTPUT}\")\n",
    "    with OUTPUT.open(\"w\", encoding=\"utf-8\") as out_f:\n",
    "        for doc in existing_docs:\n",
    "            out_f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "            total_written += 1\n",
    "        for doc in sampled_new:\n",
    "            out_f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "            total_written += 1\n",
    "\n",
    "    logger.info(f\"Wrote {total_written:,} documents to {OUTPUT} (existing: {len(existing_docs):,}, added: {len(sampled_new):,})\")\n",
    "\n",
    "    # 4) save added IDs\n",
    "    with ADDED_IDS.open(\"w\", encoding=\"utf-8\") as idf:\n",
    "        for doc in sampled_new:\n",
    "            idf.write(str(doc[\"_id\"]) + \"\\n\")\n",
    "    logger.info(f\"Saved added IDs ({len(sampled_new):,}) to {ADDED_IDS}\")\n",
    "\n",
    "    # 5) final summary\n",
    "    print()\n",
    "    print(f\"Existing docs: {len(existing_docs):,}\")\n",
    "    print(f\"Added docs: {len(sampled_new):,}\")\n",
    "    print(f\"Final total written: {total_written:,}\")\n",
    "    print(f\"Output file: {OUTPUT}\")\n",
    "    print(f\"Log: {LOG_PATH}\")\n",
    "    logger.info(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d56b931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents in new sampled corpus: 200000\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/PAR/sampled_corpus_200k.jsonl\", 'r') as data:\n",
    "    lines = data.readlines()\n",
    "    print(f\"Total documents in new sampled corpus: {len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b2d1f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c49f6",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83836f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query-id\n",
      "1208875-1    1\n",
      "1208875-2    1\n",
      "1262727-1    2\n",
      "1373662-1    1\n",
      "1373662-2    1\n",
      "Name: count_score_2, dtype: int64\n",
      "Total queries in TEST: 5900\n",
      "Total articles: 55265\n",
      "Total articles with score==2: query-id     2800\n",
      "corpus-id    2800\n",
      "score        2800\n",
      "dtype: int64\n",
      "Queries with at least one score==2: 2109\n",
      "Total rows with score==2: 2800\n"
     ]
    }
   ],
   "source": [
    "# Load the sampled 50k qrels and count per query how many 'score' == 2\n",
    "import pandas as pd\n",
    "\n",
    "train_50k = pd.read_csv(\"data/PAR/qrels_dev.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Filter rows where score == 2, then group by query-id and count\n",
    "score2_counts = train_50k[train_50k['score'] == 2].groupby('query-id').size().rename('count_score_2')\n",
    "\n",
    "# Show top rows\n",
    "print(score2_counts.head())\n",
    "\n",
    "# Summary stats\n",
    "print(f\"Total queries in TEST: {train_50k['query-id'].nunique()}\")\n",
    "print(f\"Total articles: {train_50k['corpus-id'].nunique()}\")\n",
    "print(f\"Total articles with score==2: {train_50k[train_50k['score'] == 2].count()}\")\n",
    "print(f\"Queries with at least one score==2: {score2_counts.shape[0]}\")\n",
    "print(f\"Total rows with score==2: {score2_counts.sum()}\")\n",
    "\n",
    "# Save counts to file\n",
    "# score2_counts.to_csv('data/PAR/train_50k_score2_counts.tsv', sep='\\t', header=True)\n",
    "# print(\"Saved per-query score==2 counts to data/PAR/train_50k_score2_counts.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c8399ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query-id\n",
      "1181818-1    1\n",
      "1192792-1    1\n",
      "1208925-1    1\n",
      "1386678-1    1\n",
      "1403799-1    1\n",
      "Name: count_score_2, dtype: int64\n",
      "Total queries in TEST: 5937\n",
      "Total articles: 56304\n",
      "Total articles with score==2: query-id     3088\n",
      "corpus-id    3088\n",
      "score        3088\n",
      "dtype: int64\n",
      "Queries with at least one score==2: 2150\n",
      "Total rows with score==2: 3088\n",
      "77147\n"
     ]
    }
   ],
   "source": [
    "# Load the sampled 50k qrels and count per query how many 'score' == 2\n",
    "import pandas as pd\n",
    "\n",
    "train_50k = pd.read_csv(\"data/PAR/qrels_test.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Filter rows where score == 2, then group by query-id and count\n",
    "score2_counts = train_50k[train_50k['score'] == 2].groupby('query-id').size().rename('count_score_2')\n",
    "\n",
    "# Show top rows\n",
    "print(score2_counts.head())\n",
    "\n",
    "# Summary stats\n",
    "print(f\"Total queries in TEST: {train_50k['query-id'].nunique()}\")\n",
    "print(f\"Total articles: {train_50k['corpus-id'].nunique()}\")\n",
    "print(f\"Total articles with score==2: {train_50k[train_50k['score'] == 2].count()}\")\n",
    "print(f\"Queries with at least one score==2: {score2_counts.shape[0]}\")\n",
    "print(f\"Total rows with score==2: {score2_counts.sum()}\")\n",
    "print(len(train_50k))\n",
    "\n",
    "# Save counts to file\n",
    "# score2_counts.to_csv('data/PAR/train_50k_score2_counts.tsv', sep='\\t', header=True)\n",
    "# print(\"Saved per-query score==2 counts to data/PAR/train_50k_score2_counts.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c502b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query-id\n",
      "1065068-1    1\n",
      "1079861-1    1\n",
      "1079877-1    1\n",
      "1079877-2    1\n",
      "1079877-3    1\n",
      "Name: count_score_2, dtype: int64\n",
      "Total queries in sample: 154518\n",
      "Total articles with score==2: query-id     74886\n",
      "corpus-id    74886\n",
      "score        74886\n",
      "dtype: int64\n",
      "Queries with at least one score==2: 55561\n",
      "Total rows with score==2: 74886\n",
      "Total articles:1978118\n"
     ]
    }
   ],
   "source": [
    "# Load the sampled 50k qrels and count per query how many 'score' == 2\n",
    "import pandas as pd\n",
    "\n",
    "train_50k = pd.read_csv(\"data/PAR/qrels_train.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Filter rows where score == 2, then group by query-id and count\n",
    "score2_counts = train_50k[train_50k['score'] == 2].groupby('query-id').size().rename('count_score_2')\n",
    "\n",
    "# Show top rows\n",
    "print(score2_counts.head())\n",
    "\n",
    "# Summary stats\n",
    "print(f\"Total queries in sample: {train_50k['query-id'].nunique()}\")\n",
    "print(f\"Total articles with score==2: {train_50k[train_50k['score'] == 2].count()}\")\n",
    "print(f\"Queries with at least one score==2: {score2_counts.shape[0]}\")\n",
    "print(f\"Total rows with score==2: {score2_counts.sum()}\")\n",
    "print(f\"Total articles:{train_50k['corpus-id'].shape[0]}\")\n",
    "\n",
    "# Save counts to file\n",
    "# score2_counts.to_csv('data/PAR/train_50k_score2_counts.tsv', sep='\\t', header=True)\n",
    "# print(\"Saved per-query score==2 counts to data/PAR/train_50k_score2_counts.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968d237",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cc47469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query-id\n",
      "1087841-1    1\n",
      "1087841-2    1\n",
      "1087841-3    1\n",
      "1087841-4    1\n",
      "1087841-5    1\n",
      "Name: count_score_2, dtype: int64\n",
      "Total queries in sample: 72381\n",
      "Total articles with score==2: query-id     4286\n",
      "corpus-id    4286\n",
      "score        4286\n",
      "dtype: int64\n",
      "Queries with at least one score==2: 4197\n",
      "Total rows with score==2: 4286\n"
     ]
    }
   ],
   "source": [
    "# Load the sampled 50k qrels and count per query how many 'score' == 2\n",
    "import pandas as pd\n",
    "\n",
    "train_50k = pd.read_csv(\"data/PAR/50k_qrels_train.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Filter rows where score == 2, then group by query-id and count\n",
    "score2_counts = train_50k[train_50k['score'] == 2].groupby('query-id').size().rename('count_score_2')\n",
    "\n",
    "# Show top rows\n",
    "print(score2_counts.head())\n",
    "\n",
    "# Summary stats\n",
    "print(f\"Total queries in sample: {train_50k['query-id'].nunique()}\")\n",
    "print(f\"Total articles with score==2: {train_50k[train_50k['score'] == 2].count()}\")\n",
    "print(f\"Queries with at least one score==2: {score2_counts.shape[0]}\")\n",
    "print(f\"Total rows with score==2: {score2_counts.sum()}\")\n",
    "\n",
    "# Save counts to file\n",
    "# score2_counts.to_csv('data/PAR/train_50k_score2_counts.tsv', sep='\\t', header=True)\n",
    "# print(\"Saved per-query score==2 counts to data/PAR/train_50k_score2_counts.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6488ffdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query-id     74886\n",
       "corpus-id    74886\n",
       "score        74886\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_50k[train_50k['score'] == 2].count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mich",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
